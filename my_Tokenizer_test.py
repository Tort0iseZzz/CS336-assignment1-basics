import os
import regex as re
from cs336_basics.bpe_tokenizer import train_bpe, save_tokenizer_assets, Tokenizer

if __name__ == "__main__":
    VOCAB_SAVE_PATH = "models/tokenizer/vocab.json"
    MERGES_SAVE_PATH = "models/tokenizer/merges.txt"
    SPECIAL_TOKENS = ["<|endoftext|>"]

    # åŠ è½½åˆšåˆšè®­ç»ƒå¥½çš„åˆ†è¯å™¨
    print(f"ğŸš€ æ­£åœ¨æ ¹æ®æ•°æ® {VOCAB_SAVE_PATH} å’Œ {MERGES_SAVE_PATH} åˆ›å»ºTokenizer...")
    tokenizer = Tokenizer.from_files(VOCAB_SAVE_PATH, MERGES_SAVE_PATH, SPECIAL_TOKENS)

    test_str = " loved" # æ³¨æ„å‰é¢æœ‰ä¸€ä¸ªç©ºæ ¼
    # æ‰“å°é¢„åˆ†è¯ç»“æœ
    for match in re.finditer(tokenizer.PAT, test_str):
        print(f"{test_str} çš„é¢„åˆ†è¯ç‰‡æ®µ: '{match.group()}'")

    # æµ‹è¯•ä¸€æ®µ TinyStories ä¸­çš„å…¸å‹æ–‡æœ¬
    TEST_TEXT1 = "Once upon a time, there was a little girl named LilyğŸ˜Š."
    TEST_TEXT2 = " loved"
    TEST_TEXT3 = "a sdhueihfiegfwyuegd dhdu sdoi i so do this\n<|endoftext|>"
    test_text = TEST_TEXT3
    ids = tokenizer.encode(test_text)
    tokens = [tokenizer.decode([i]) for i in ids]

    print(f"åŸå§‹æ–‡æœ¬: {test_text}")
    print(f"idç»“æœ: {ids}")
    print(f"åˆ‡åˆ†ç»“æœ: {tokens}")
    print(f"å‹ç¼©æ¯”: {len(test_text) / len(ids):.2f} (å­—ç¬¦/Token)")