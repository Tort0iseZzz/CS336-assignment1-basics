import os
from cs336_basics.bpe_tokenizer import train_bpe, save_tokenizer_assets, Tokenizer

if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    INPUT_FILE_LARGE = "data/TinyStoriesV2-GPT4-train.txt"
    INPUT_FILE_SMALL = "data/TinyStoriesV2-GPT4-valid.txt"
    INPUT_FILE = INPUT_FILE_LARGE
    VOCAB_SAVE_PATH = "models/tokenizer/vocab.json"
    MERGES_SAVE_PATH = "models/tokenizer/merges.txt"
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(VOCAB_SAVE_PATH), exist_ok=True)

    # è®­ç»ƒé…ç½®
    # æ³¨æ„ï¼šTinyStories å¾ˆå¤§ï¼Œå¦‚æœå…¨é‡è®­ç»ƒå¤ªæ…¢ï¼Œå¯ä»¥å…ˆç”¨ä¸€ä¸ªå°å­é›†æµ‹è¯•
    VOCAB_SIZE = 32768
    SPECIAL_TOKENS = ["<|endoftext|>"]
    NUM_PROCESSES=32

    print(f"ğŸš€ å¼€å§‹åœ¨ {INPUT_FILE} ä¸Šè®­ç»ƒ BPE...")
    
    # è°ƒç”¨ä½ å®ç°çš„ train_bpe
    vocab, merges = train_bpe(
        input_path=INPUT_FILE,
        vocab_size=VOCAB_SIZE,
        special_tokens=SPECIAL_TOKENS,
        num_processes=NUM_PROCESSES
    )

    # ä¿å­˜ç»“æœ
    print(f"ğŸš€ è®­ç»ƒå®Œæˆï¼æ­£åœ¨ä¿å­˜ç»“æœåˆ° {VOCAB_SAVE_PATH} å’Œ {MERGES_SAVE_PATH}...")
    save_tokenizer_assets(vocab, merges, VOCAB_SAVE_PATH, MERGES_SAVE_PATH)
    print("ä¿å­˜å®Œæˆï¼")